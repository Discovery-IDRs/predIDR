{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio.SeqIO as SeqIO\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sym_codes = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "             'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ohc(seq):\n",
    "    \"\"\"\n",
    "    One Hot Encodes the categorical data of the string of amino acids passed into \n",
    "    a 2D list of numerical data.\n",
    "\n",
    "    :param seq: amino acid sequence string\n",
    "    \n",
    "    :return: one hot encoded sequence \n",
    "    \"\"\"\n",
    "    \n",
    "    seq_idx = [sym_codes.index(sym) for sym in seq]\n",
    "    x = np.array(seq_idx)\n",
    "    x = keras.utils.to_categorical(x, num_classes=len(sym_codes), dtype='int32')\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seqs_path, label_path):\n",
    "    \"\"\"\n",
    "    Given the path to the fasta files of the sequence and labels\n",
    "    returns a list with tuples for each record of the weight, target and context\n",
    "\n",
    "    :param seqs_path: file path to fasta file with amino acid sequence data\n",
    "    :param labels_path: file path to fasta file with binary label data\n",
    "    \n",
    "    :return: a list of tuples with the weight, target, and context for each record\n",
    "            ex: [(weight, target, context), ]\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for record_seq, record_label in zip(SeqIO.parse(seqs_path, 'fasta'), SeqIO.parse(label_path, 'fasta')):\n",
    "        \n",
    "        # one hot encode each record_seq \n",
    "        seq = str(record_seq.seq)\n",
    "        seq_ohc = convert_ohc(seq)\n",
    "        \n",
    "        # expand the dimension of record_label for broadcasting\n",
    "        label = [int(sym) for sym in record_label]\n",
    "        weight = np.expand_dims(label, axis = 1)\n",
    "        \n",
    "        # get the target from the record \n",
    "        target = weight*seq_ohc\n",
    "        \n",
    "        # get the context from the record (inverted the weight)\n",
    "        context = (np.invert(weight) + 2)*seq_ohc \n",
    "        \n",
    "        data.append((label, target, context))\n",
    "        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = load_data('../../inpainting_mobidb/out/train_seq.fasta', '../../inpainting_mobidb/out/train_label.fasta')\n",
    "\n",
    "test_data = load_data('../../inpainting_mobidb/out/test_seq.fasta','../../inpainting_mobidb/out/test_label.fasta')\n",
    "\n",
    "valid_data = load_data('../../inpainting_mobidb/out/validation_seq.fasta','../../inpainting_mobidb/out/validation_label.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set: 1495\n",
      "Length of testing set: 187\n",
      "Length of validation set: 186\n"
     ]
    }
   ],
   "source": [
    "# Size and shape of training, testing, and validation sets\n",
    "train_len = len(train_data)\n",
    "test_len = len(test_data)\n",
    "valid_len = len(valid_data)\n",
    "print(\"Length of training set: \" + str(train_len))\n",
    "print(\"Length of testing set: \" + str(test_len))\n",
    "print(\"Length of validation set: \" + str(valid_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make generative model\n",
    "def make_generative_model():\n",
    "    # convolution \n",
    "    model = tensorflow.keras.Sequential()\n",
    "    model.add(keras.Input(shape=((180, 20))))\n",
    "    \n",
    "    # MAYBE 40?????\n",
    "    model.add(keras.layers.Conv1D(8, 3, strides = 1, padding='same', name='first'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    model.add(keras.layers.Conv1D(16, 3, strides = 1, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    model.add(keras.layers.Conv1D(32, 3, strides = 1, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "\n",
    "    model.add(keras.layers.Conv1D(64, 3, strides = 1, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    model.add(keras.layers.Conv1D(128, 3, strides = 1, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    model.add(keras.layers.Conv1D(256, 3, strides = 1, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    # deconvolution \n",
    "    model.add(keras.layers.Conv1DTranspose(128, 3, strides = 1, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    model.add(keras.layers.Conv1DTranspose(64, 3, strides = 1, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    model.add(keras.layers.Conv1DTranspose(32, 3, strides = 1, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    model.add(keras.layers.Conv1DTranspose(16, 3, strides = 1, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    model.add(keras.layers.Conv1DTranspose(8, 3, strides = 1, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    #PLAY AROUND WITH THE RATIO OF FILTER \n",
    "    \n",
    "    model.add(keras.layers.Conv1DTranspose(20, 3, strides = 1, padding='same', activation = 'softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "first (Conv1D)               (None, 180, 8)            488       \n",
      "_________________________________________________________________\n",
      "batch_normalization_147 (Bat (None, 180, 8)            32        \n",
      "_________________________________________________________________\n",
      "re_lu_147 (ReLU)             (None, 180, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 180, 16)           400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_148 (Bat (None, 180, 16)           64        \n",
      "_________________________________________________________________\n",
      "re_lu_148 (ReLU)             (None, 180, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 180, 32)           1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_149 (Bat (None, 180, 32)           128       \n",
      "_________________________________________________________________\n",
      "re_lu_149 (ReLU)             (None, 180, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 180, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_150 (Bat (None, 180, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu_150 (ReLU)             (None, 180, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 180, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_151 (Bat (None, 180, 128)          512       \n",
      "_________________________________________________________________\n",
      "re_lu_151 (ReLU)             (None, 180, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 180, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_152 (Bat (None, 180, 256)          1024      \n",
      "_________________________________________________________________\n",
      "re_lu_152 (ReLU)             (None, 180, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_78 (Conv1DT (None, 180, 128)          98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_153 (Bat (None, 180, 128)          512       \n",
      "_________________________________________________________________\n",
      "re_lu_153 (ReLU)             (None, 180, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_79 (Conv1DT (None, 180, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_154 (Bat (None, 180, 64)           256       \n",
      "_________________________________________________________________\n",
      "re_lu_154 (ReLU)             (None, 180, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_80 (Conv1DT (None, 180, 32)           6176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_155 (Bat (None, 180, 32)           128       \n",
      "_________________________________________________________________\n",
      "re_lu_155 (ReLU)             (None, 180, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_81 (Conv1DT (None, 180, 16)           1552      \n",
      "_________________________________________________________________\n",
      "batch_normalization_156 (Bat (None, 180, 16)           64        \n",
      "_________________________________________________________________\n",
      "re_lu_156 (ReLU)             (None, 180, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_82 (Conv1DT (None, 180, 8)            392       \n",
      "_________________________________________________________________\n",
      "batch_normalization_157 (Bat (None, 180, 8)            32        \n",
      "_________________________________________________________________\n",
      "re_lu_157 (ReLU)             (None, 180, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_83 (Conv1DT (None, 180, 20)           500       \n",
      "=================================================================\n",
      "Total params: 266,628\n",
      "Trainable params: 265,124\n",
      "Non-trainable params: 1,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generative_model()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 20)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1418 predict_step\n        return self(x, training=False)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:196 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer sequential_13 is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: [None, 20]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-8dd9fb73c660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgenerated_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1418 predict_step\n        return self(x, training=False)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:196 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer sequential_13 is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: [None, 20]\n"
     ]
    }
   ],
   "source": [
    "generator = make_generative_model()\n",
    "\n",
    "#masked = x_train_mask[0]\n",
    "\n",
    "train_img = train[0][1]\n",
    "\n",
    "generated_image = generator.predict(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 180, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make discrimator model\n",
    "def make_discriminator_model():\n",
    "    model = tensorflow.keras.Sequential()\n",
    "    model.add(keras.Input(shape=((180, 20))))\n",
    "    \n",
    "    model.add(keras.layers.Conv1D(25, 4, strides = 2, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    model.add(keras.layers.Conv1D(13, 4, strides = 2, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    model.add(keras.layers.Conv1D(7, 4, strides = 2, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    model.add(keras.layers.Conv1D(4, 4, strides = 2, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    \n",
    "    #model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1, activation = 'softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 90, 25)            2025      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 90, 25)            100       \n",
      "_________________________________________________________________\n",
      "re_lu_22 (ReLU)              (None, 90, 25)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 45, 13)            1313      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 45, 13)            52        \n",
      "_________________________________________________________________\n",
      "re_lu_23 (ReLU)              (None, 45, 13)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 23, 7)             371       \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 23, 7)             28        \n",
      "_________________________________________________________________\n",
      "re_lu_24 (ReLU)              (None, 23, 7)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 12, 4)             116       \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 12, 4)             16        \n",
      "_________________________________________________________________\n",
      "re_lu_25 (ReLU)              (None, 12, 4)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12, 1)             5         \n",
      "=================================================================\n",
      "Total params: 4,026\n",
      "Trainable params: 3,928\n",
      "Non-trainable params: 98\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tensorflow.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, sample_weight):\n",
    "    # label: data_step[0], target: data_step[1], context: data_step[2]\n",
    "    cross_entropy(tensorflow.ones_like(fake_output), fake_output, sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output, data_step):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output, sample_weight)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output, sample_weight)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tensorflow.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tensorflow.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(data_step):\n",
    "    # label: data_step[0], target: data_step[1], context: data_step[2]\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_target = generator(data_step[2], training=True)\n",
    "        \n",
    "        real_output = discriminator(data_step[1], training=True)\n",
    "        fake_output = discriminator(generated_target, training=True)\n",
    "        \n",
    "        \n",
    "        # need to incorporate weights, labels \n",
    "        # need to have target in the generator loss\n",
    "        gen_loss = generator_loss(fake_output, data_step[0])\n",
    "        disc_loss = discriminator_loss(real_output, fake_output, data_step[0])\n",
    "    \n",
    "    #backpropogration \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for data_step in data:\n",
    "            train_step(data_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do I run validation batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def load_data(seqs_path, labels_path):\n",
    "    \"\"\"\n",
    "    Given the path to the fasta files of the sequence and labels\n",
    "    returns a list of a tuple pair of the accession of sequence/label\n",
    "    and a string of sequence/string.\n",
    "\n",
    "    :param seqs_path: file path to fasta file with amino acid sequence data\n",
    "    :param labels_path: file path to fasta file with binary label data\n",
    "    \n",
    "    :return: [(accession_seq, 'amino_acid_seq')], [(accession_label, 'label')]\n",
    "            ex: [('QP106', 'AASSSDD'), ...], [('QP106', '00001111000'), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # Load files\n",
    "    seqs = []\n",
    "    for record in SeqIO.parse(seqs_path, 'fasta'):\n",
    "        #accession = record.description.split('|')[0]\n",
    "        seq = str(record.seq)\n",
    "        seqs.append((accession, seq))\n",
    "\n",
    "    labels = []\n",
    "    for record in SeqIO.parse(labels_path, 'fasta'):\n",
    "        #accession = record.description.split('|')[0]\n",
    "        label = [int(sym) for sym in record.seq]\n",
    "        labels.append((accession, label))\n",
    "\n",
    "    return seqs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def convert_ohc(seqs):\n",
    "    \"\"\"\n",
    "    Converts the given tuple of the accession and amino acid sequence string into\n",
    "    a int list using sym_codes and then one hot encoded the int list using keras.util.to_categorical\n",
    "\n",
    "    :param seqs: list of tuple pair of the accession of the amino acid sequence\n",
    "    and the string of the amino acid sequence ex: [('QP106', 'AASSSDD'), ...]\n",
    "    :return: 2D list of each one hot encoded amino acid sequence ex: [[0,0,0,0,0,0,1], ...]\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    # convert string amino acid string characters into int list\n",
    "    for _, seq in seqs:\n",
    "        seq_idx = [sym_codes.index(sym) for sym in seq]\n",
    "        x.append(seq_idx)\n",
    "\n",
    "    # convert list into numpy array and one hot encode int array\n",
    "    x = np.array(x)\n",
    "    x = keras.utils.to_categorical(x, num_classes=len(sym_codes))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert character array to numpy array and one hot encode the sequences\n",
    "x_train = convert_ohc(train_seqs)\n",
    "x_test = convert_ohc(test_seqs)\n",
    "x_valid = convert_ohc(valid_seqs)\n",
    "\n",
    "y_train = np.array([label for _, label in train_labels])\n",
    "y_test = np.array([label for _, label in test_labels])\n",
    "y_valid = np.array([label for _, label in valid_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mask the sequences\n",
    "x_train_mask = np.expand_dims(y_train, axis=2)*x_train\n",
    "x_test_mask = np.expand_dims(y_test, axis=2)*x_test\n",
    "x_valid_mask = np.expand_dims(y_valid, axis=2)*x_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target for sequences \n",
    "x_train_target = np.expand(np.invert(y_train[0]) + 2)*x_train \n",
    "x_test_target = np.expand(np.invert(y_test[0]) + 2)*x_train \n",
    "x_valid_mask = np.expand(np.invert(y_valid[0]) + 2)*x_train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masked = x_train_mask[0:5]\n",
    "masked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generator_loss(fake_output, sample_weights):\n",
    "    return cross_entropy(tensorflow.ones_like(fake_output), fake_output) + cross_entropy(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_step(masked_seq, target, sample_weights):\n",
    "    #noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_target = generator(masked_seq, training=True)\n",
    "        \n",
    "        real_output = discriminator(target, training=True)\n",
    "        fake_output = discriminator(generated_target, training=True)\n",
    "        \n",
    "        \n",
    "        # need to incorporate weights, labels \n",
    "        # need to have target in the generator loss\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    #backpropogration \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
